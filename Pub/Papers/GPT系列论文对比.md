---
title: GPT系列论文对比
date: 2023-04-14
tags : [
	"论文阅读笔记",
]
categories : [
	"论文阅读笔记",
]
series : []
aliases : []
draft: false
toc: true
---
论文：
1. GPT：Improving Language Understanding by Generative Pre-Training
2. GPT2：Language Models are Unsupervised Multitask Learners
3. GPT3原始论文：Language Models are Few-Shot Learners

# 第一遍 ： 阅读标题、摘要、总结
> 明确是否要继续读下去

| 类型     | 答案 |
| :-------- | :---- |
| 方向     |  GPT系列论文对比。分析不同论文之间的共性、差异性    |
| 大概方法 |      |
| 结论     |      |

![[Pics/Pasted image 20230414093150.png]]
GPT的Decoder去除了一层的masked 注意力层。并在GPT2后，修改了Norm的位置。
| 模型 | 架构                                                                                         | 数据量                                           | 训练方式                                                                                                                                                                                                                                                               | 训练时间                   | 主要区别 |
| ---- | -------------------------------------------------------------------------------------------- | ---------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------- | ---- |
| GPT  | 1.17亿参数量。Trm层数12层，多头个数12个，隐藏层768。max length:512.                                        | BooksCorpus：7000本书籍。大小：5GB             | **无监督训练+有监督finetune**。无监督做下一个词预测，求句子的最大似然概率。有监督微调时根据下游任务来，如问答和常识推理任务，首先将背景信息与问题进行拼接，然后再将拼接后的文本依次与每个答案进行拼接，最后依次传入Transformer模型，最后接一层线性层得多每个输入的预测值。 |            没提                |      |
| GPT2 | 15亿参数量。Trm层数48层，多头个数12个，隐藏层1600。词汇表数量：50257。max length:1024, batchsize:512。局部修改，添加了一层归一化，移动原本归一化层位置。 | WebText， 由Reddit上搜集的800万个网页，大小40G |**无监督训练（下一个词预测）+不做微调**。不需要任何参数或架构的修改，就能在诸多任务上取得较好的结果。                                                                                                                                                                                                                                                                        | 32个TPU训练一周。4.3万美金 |  相比于gpt1扩大了模型参数量，微调了结构。并且取消了下游任务层，首次尝试了zero-shot，并在众多任务上取得了不错的效果。提出“任何有监督任务都是语言模型的一个子集，当模型的容量非常大且数据量足够丰富时，仅仅靠训练语言模型的学习便可以完成其他有监督学习的任务” 。但大部分zero-shot的效果仍然落后于有监督。  |
| GPT3 | 1750亿参数量。Trm层96层，多头个数96.隐藏层12888，max length:2048                                                                               | 570GB过滤后的数据。低质量的Common Crawl，高质量的WebText2, Books1, Books2, Wikipedia。根据数据质量不同赋予不同的训练权值。                                      |   **不再微调**。提出了In-context learning的概念。即情景学习，不再针对某一种任务进行梯度下降，而是学习上下文的语境。根据提示样本的多少，分为了zero-shot, one-shot, few-shot. 使用了稀疏注意模式。                                                                                                                                                                                                                                                               |   285,000个cpu，10,000个GPU，1200万美元训练费用。                         | 提出了In-context learning的概念。相比于纯粹的无微调，引入了部分背景知识帮助模型解答问题。相关研究表明，这种方式并没有让模型学习，而是激活了模型语言表达的风格，prompt建立了文本理解和完成任务之间的桥梁。     |

结论：
1. 随着模型的参数量继续扩大，仍然没有触碰到模型性能的波峰。
2. 随着模型参数量的扩大，模型的泛化理解能力是越来越强的。模型缺的只是如何来将理解的内容给表达出来。
3. 模型越来越大，普通人玩不起。


## GPT3 in-context learning
![[Pics/Pasted image 20230414153254.png]]
1. few-shot learning是不做训练的
2. 对于分类问题。使用True或者False
3. 对于生成式问题，采用Beam Search搜索一个答案。
4. 数据量和计算量的关系。参考李沐视频1:18


## 参考资料
1. https://zhuanlan.zhihu.com/p/430749548
2. [incontext-learning究竟学了什么？](https://zhuanlan.zhihu.com/p/484999828)




# 第二遍 ：厘清文章脉络，知道在讲什么、怎么做的、做的怎么样
  
## 困境：
## 贡献：
## 背景：
## 方法：
## 实验：
## 问题：
## 想法：

# 第三遍 ：作者视角思考论文


| 类型                                           | 答案 |
|:---------------------------------------------- |:---- |
| 论文框架                                       |      |
| 想法怎么产生的？（如何面对困境，如何引入方法） |      |
| 实验怎么设计的？                               |      |
| 哪些部分对效果提升的最大？                     |      |
| 得到的经验                    |      |
