---
title: GPT系列论文对比
date: 2023-04-14
tags : [
	"论文阅读笔记",
]
categories : [
	"论文阅读笔记",
]
series : []
aliases : []
draft: false
toc: true
---
论文：
1. GPT：Improving Language Understanding by Generative Pre-Training
2. GPT2：Language Models are Unsupervised Multitask Learners
3. GPT3原始论文：Language Models are Few-Shot Learners

# 第一遍 ： 阅读标题、摘要、总结
> 明确是否要继续读下去

| 类型     | 答案 |
| :-------- | :---- |
| 方向     |  GPT系列论文对比。分析不同论文之间的共性、差异性    |
| 大概方法 |      |
| 结论     |      |

![[Pics/Pasted image 20230414093150.png]]
GPT的Decoder去除了一层的masked 注意力层。并在GPT2后，修改了Norm的位置。
| 模型 | 架构                                                                                         | 数据量                                           | 训练方式                                                                                                                                                                                                                                                               | 训练时间                   | 主要区别 |
| ---- | -------------------------------------------------------------------------------------------- | ---------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------- | ---- |
| GPT  | 1.17亿参数量。Trm层数12层，隐藏层768。max length:512.                                        | BooksCorpus：7000本书籍。大小：5GB             | **无监督训练+有监督finetune**。无监督做下一个词预测，求句子的最大似然概率。有监督微调时根据下游任务来，如问答和常识推理任务，首先将背景信息与问题进行拼接，然后再将拼接后的文本依次与每个答案进行拼接，最后依次传入Transformer模型，最后接一层线性层得多每个输入的预测值。 |                            |      |
| GPT2 | 15亿参数量。Trm层数48层，隐藏层维度1600。词汇表数量：50257。max length:1024, batchsize:512。局部修改，添加了一层归一化，移动原本归一化层位置。 | WebText， 由Reddit上搜集的800万个网页，大小40G |                                                                                                                                                                                                                                                                        | 32个TPU训练一周。4.3万美金 |      |
| GPT3 | 1750亿参数量。Trm层96层，多头个数96.隐藏层长度12888，max length:2048                                                                               | 570GB过滤后的数据。低质量的Common Crawl，高质量的WebText2, Books1, Books2, Wikipedia。根据数据质量不同赋予不同的训练权值。                                      |                                                                                                                                                                                                                                                                        |   285,000个cpu，10,000个GPU，1200万美元训练费用。                         |      |


## 参考资料
1. https://zhuanlan.zhihu.com/p/430749548


# 第二遍 ：厘清文章脉络，知道在讲什么、怎么做的、做的怎么样
  
## 困境：
## 贡献：
## 背景：
## 方法：
## 实验：
## 问题：
## 想法：

# 第三遍 ：作者视角思考论文


| 类型                                           | 答案 |
|:---------------------------------------------- |:---- |
| 论文框架                                       |      |
| 想法怎么产生的？（如何面对困境，如何引入方法） |      |
| 实验怎么设计的？                               |      |
| 哪些部分对效果提升的最大？                     |      |
| 得到的经验                    |      |
