---
doc_type: hypothesis-highlights
url: >-
  https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/Adaboost.md
---

# daily-interview/机器学习基础面试题.md at master · datawhalechina/daily-interview

## Metadata
- Author: [github.com]()
- Title: daily-interview/机器学习基础面试题.md at master · datawhalechina/daily-interview
- Reference: https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/Adaboost.md
- Category: #article

## Page Notes
## Highlights
- 不管是分类问题还是回归问题，根据误差改变权重就是Adaboost的本质，可以基于这个构建相应的强学习器。 — [Updated on 2022-02-26 18:38:30](https://hyp.is/PPl_ZpbwEeymGXfxmgX-Iw/github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/Adaboost.md) — Group: #self-notation

- ​ Adaboost每一个根据前m-1个模型的误差更新当前数据集的权重，学习第m个学习器 — [Updated on 2022-02-26 18:39:35](https://hyp.is/Y-d12JbwEeyx7DeAiVmZkA/github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/Adaboost.md) — Group: #self-notation

- GBDT是根据前m-1个的学习剩下的label的偏差，修改当前数据的label进行学习第m个学习器，一般使用梯度的负方向替代偏差进行计算。 — [Updated on 2022-02-26 18:39:37](https://hyp.is/ZVM8aJbwEeynMOMKRYgZVQ/github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/Adaboost.md) — Group: #self-notation

- 会把样本权重高的样本分类正确。这样造成的结果是，虽然每个弱分类器可能都有分错的样本，然而整个 AdaBoost 却能保证对每个样本进行正确分类，从而实现快速收敛。 — [Updated on 2022-02-26 18:40:48](https://hyp.is/j0GWbpbwEey3QaPugl_69w/github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/Adaboost.md) — Group: #self-notation

- 选择简单的逻辑回归 — [Updated on 2022-02-26 18:48:36](https://hyp.is/pqn-ipbxEeymOqvRiOCV5g/github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/Adaboost.md) — Group: #self-notation

- 回归问题选择线性回归 — [Updated on 2022-02-26 18:48:39](https://hyp.is/p_61FJbxEeyxCUP4WRbSfQ/github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/Adaboost.md) — Group: #self-notation




