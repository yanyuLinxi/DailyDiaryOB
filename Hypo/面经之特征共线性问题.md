---
doc_type: hypothesis-highlights
url: 'https://zhuanlan.zhihu.com/p/70124378'
---

# 面经之特征共线性问题

## Metadata
- Author: [zhuanlan.zhihu.com]()
- Title: 面经之特征共线性问题
- Reference: https://zhuanlan.zhihu.com/p/70124378
- Category: #article

## Page Notes
## Highlights
- 多重共线性（Multicollinearity）是指线性回归模型中的自变量之间由于存在高度相关关系而使模型的权重参数估计失真或难以估计准确的一种特性，多重是指一个自变量可能与多个其他自变量之间存在相关关系。 — [Updated on 2022-03-03 09:33:12](https://hyp.is/46HouJqREey7Tf9DSns_Qw/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- 你的两个自变量间（X1和X2）存在函数关系，那么X1改变一个单位时，X2也会相应地改变，此时你无法做到固定其他条件，单独考查X1对因变量Y的作用，你所观察到的X1的效应总是混杂了X2的作用，这就造成了分析误差，使得对自变量效应的分析不准确， — [Updated on 2022-03-03 09:33:30](https://hyp.is/7tg2zpqREeyy0a9SgZxKAQ/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- 如果自变量存在共线性，将无法区分它们对因变量的影响 — [Updated on 2022-03-03 09:33:46](https://hyp.is/-F7gvJqREeyVtb9ja74f7Q/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- 但是有时候做预测（prediction）时，我们并不关心如何解释自变量对因变量的影响 — [Updated on 2022-03-03 09:34:00](https://hyp.is/AD1pDJqSEey7T_fzpH__mA/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- 做预测时，往往用贪婪算法进行变量选择，只有新变量对结果影响比较大时 — [Updated on 2022-03-03 09:34:37](https://hyp.is/Fjqm8pqSEeymWFPFIdeVHA/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- 每一个树的构建都是贪婪的，因此，冗余的特征并不会被加入模型中，也就是说如果变量之间相关性非常强最终很可能只会选择部分进入模型。 — [Updated on 2022-03-03 09:34:46](https://hyp.is/HBlHIpqSEeyrvvOhO2MOHg/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- ，但是这个答案的前提假设是输入的数据是理想化的，不存在噪声的 — [Updated on 2022-03-03 09:35:14](https://hyp.is/LICd9JqSEeyR7nfxSS0BEQ/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- 征共线性不影响模型的预测效果只要模型能够最终收 — [Updated on 2022-03-03 09:35:20](https://hyp.is/L-7G0pqSEeyXkR87XqB2hg/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- 如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果 — [Updated on 2022-03-03 09:35:30](https://hyp.is/Nf2NfpqSEeyOPL-xRgFCmw/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- 练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一 — [Updated on 2022-03-03 09:35:40](https://hyp.is/O9SslpqSEeynuHNCC4u-lw/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- 其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征所带来的贡献是一样的 — [Updated on 2022-03-03 09:35:51](https://hyp.is/QnjAvpqSEeyR75OhAfIxpw/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- 相关性强的特征如果同时建模，潜在的实际上是增大了模型受噪声干扰的面 — [Updated on 2022-03-03 09:36:31](https://hyp.is/Wm9mvpqSEeyISxdEU5hMYw/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- 去掉高度相关的特征会让模型的可解释性更好 — [Updated on 2022-03-03 09:36:43](https://hyp.is/YZy8mJqSEeynurcAbsw87w/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- 可以大大提高训练的速度。 — [Updated on 2022-03-03 09:36:45](https://hyp.is/YqkXCJqSEeyr4kMvDTE2vw/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- 因为有三个特征的相关性太高，训练的时候，每一颗树都是从这3个特征中随机选择一个 — [Updated on 2022-03-03 09:36:58](https://hyp.is/aqxLoJqSEey5m5No4cJQWg/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- 相关性高的特征太多放大了噪声的作用，高相关的特征往往呈现出同样的变动模式，典型的例子就是我们对于某个特征进行了大量的特征多个特征实际上“平分“了这类特征对模型的贡献，这样就导致模型对于数据的变动更加敏感，泛化误差增大； — [Updated on 2022-03-03 09:38:44](https://hyp.is/qbEg0pqSEeyk9helw3TOKA/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation

- 变量的分析造成影响，衡量变量的重要性或贡献的时候存在困难 — [Updated on 2022-03-03 09:38:49](https://hyp.is/rPUiwJqSEey8edP8F4ljBQ/zhuanlan.zhihu.com/p/70124378) — Group: #self-notation




